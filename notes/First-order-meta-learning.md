## [On First-Order Meta-Learning Algorithms](https://arxiv.org/abs/1803.029999)

TLDR; 

### Key Points
- Tenenbaum and collaborators, argues that humansâ€™ fast-learning abilities can
be explained as Bayesian inference, and that the key to developing algorithms with human-level learning speed is to make our algorithms more Bayesian.
- Meta-learning emerges from an approach to use for learning from a small amount of data. Instead of trying to do Bayesian inference which may be intractable.
- In this work meta-learning algorithms based on the first order gradient information is explored. Introduced is the algorithm MAML (Model-Agnostic Meta-Learning), where later First Order MAML, FOMAML, was introduced obtained nearly as well performance as MAML on ImageNet while ignoring the second-order gradient information.

### Notes