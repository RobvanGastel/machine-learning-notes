#### 2021-07
- Neural Architecture Search
    - sharpDARTS: Faster and More Accurate Differentiable Architecture Search [[arXiv](https://arxiv.org/abs/1903.09900)]
    - Progressive Neural Architecture Search [[arXiv](https://arxiv.org/abs/1712.00559)]
    - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [[arXiv](https://arxiv.org/abs/1811.06965)]
    - Xception: Deep Learning with Depthwise Separable Convolutions [[arXiv](https://arxiv.org/abs/1610.02357)]
    - SGDR: Stochastic Gradient Descent with Warm Restarts [[arXiv](https://arxiv.org/abs/1608.03983v5)]
    - MobileNetV2: Inverted Residuals and Linear Bottlenecks [[arXiv](https://arxiv.org/abs/1801.04381)]
- Few-Shot Learning
    - Optimization as a Model for Few-Shot Learning [[OpenReview](https://openreview.net/forum?id=rJY0-Kcll)]
    - Learning to Optimize Neural Nets [[arXiv](https://arxiv.org/abs/1703.00441)]
    - A Broader Study of Cross-Domain Few-Shot Learning [[arXiv](https://arxiv.org/abs/1912.07200)]
    - Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation [[arXiv](https://arxiv.org/abs/2001.08735)]
    - Towards Understanding Generalization in Gradient-Based Meta-Learning [[arXiv](https://arxiv.org/abs/1907.07287)]
- Meta-Learning
    - ProMP: Proximal Meta-Policy Search [[arXiv](https://arxiv.org/abs/1810.06784)]
    - Some Considerations on Learning to Explore via Meta-Reinforcement Learning [[arXiv](https://arxiv.org/abs/1803.01118)]
- Reinforcement Learning
    - Reinforcement Learning with Augmented Data [[arXiv](https://arxiv.org/abs/2004.14990)]

#### 2021-06
- Neural Architecture Search
    - Measuring Robustness to Natural Distribution Shifts in Image Classification [[arXiv](https://arxiv.org/abs/2007.00644)]
    - Is it Enough to Optimize CNN Architectures on ImageNet? [[arXiv](https://arxiv.org/abs/2103.09108)]
    - Do ImageNet Classifiers Generalize to ImageNet? [[paper](http://proceedings.mlr.press/v97/recht19a/recht19a.pdf)]
    - SNAS: Stochastic Neural Architecture Search [[arXiv](https://arxiv.org/abs/1812.09926)]
- Meta-Learning
    - Matching Networks for One Shot Learning [[arXiv](https://arxiv.org/abs/1606.04080)]
    - Meta-learning with negative learning rates [[arXiv](https://arxiv.org/abs/2102.00940)]
    - Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML [[arXiv](https://arxiv.org/abs/1909.09157)]
    - Performance-Weighed Policy Sampling for Meta-Reinforcement Learning [[arXiv](https://arxiv.org/abs/2012.06016)]
    - Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? [[arXiv](https://arxiv.org/abs/2003.11539)]
- Computer Vision
    - How Useful is Self-Supervised Pretraining for Visual Tasks? [[arXiv](https://arxiv.org/abs/2003.14323)]
    - Improved Regularization of Convolutional Neural Networks with Cutout [[arXiv](https://arxiv.org/abs/1708.04552)]
    - Attention Is All You Need [[arXiv](https://arxiv.org/abs/1706.03762)]
- Reinforcement Learning
    - Towards Sample Efficient Reinforcement Learning [[IJCAI](https://www.ijcai.org/proceedings/2018/820)]
    - Measuring Progress in Deep Reinforcement Learning Sample Efficiency [[arXiv](https://arxiv.org/abs/2102.04881)]
- A Baseline for Few-Shot Image Classification [[arXiv](https://arxiv.org/abs/1909.02729)]
- Gradient Estimation Using Stochastic Computation Graphs [[arXiv](https://arxiv.org/abs/1506.05254)]
- Categorical Reparameterization with Gumbel-Softmax [[arXiv](https://arxiv.org/abs/1611.01144)]
- A Metric Learning Reality Check [[arXiv](https://arxiv.org/abs/2003.08505)]

#### 2021-05
- Neural Architecture Search
    - Drawing early-bird tickets: Towards more efficient training of deep networks [[arXiv](https://arxiv.org/abs/1909.11957)]
- Meta-Learning
    - Towards learning-to-learn [[arXiv](https://arxiv.org/abs/1811.00231)]
    - Meta-Gradient Reinforcement Learning [[arXiv](https://arxiv.org/abs/1805.09801)]
    - Torchmeta: A Meta-Learning library for PyTorch [[arXiv](https://arxiv.org/abs/1909.06576)]
- Generative Models
    - Information Theory for Intelligent People [[paper](http://tuvalu.santafe.edu/~simon/it.pdf)]
    - Approximating discrete probability distributions with dependence trees [[IEEE](https://ieeexplore.ieee.org/document/1054142)]
- Reinforcement Learning
    - Hindsight Experience Replay [[arXiv](https://arxiv.org/abs/1707.01495)]
    - High-Dimensional Continuous Control Using Generalized Advantage Estimation [[arXiv](https://arxiv.org/abs/1506.02438)]

#### 2021-04
- Neural Architecture Search
    - Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild [[arXiv](https://arxiv.org/abs/1912.10952)]
    - GOLD-NAS: Gradual, One-Level, Differentiable [[arXiv](https://arxiv.org/abs/2007.03331)]
    - Population Based Training of Neural Networks [[arXiv](https://arxiv.org/abs/1711.09846)]
    - EfficientPose: Efficient Human Pose Estimation with Neural Architecture Search [[arXiv](https://arxiv.org/abs/2012.07086)]
    - NAS evaluation is frustratingly hard [[arXiv](https://arxiv.org/abs/1912.12522)]
    - Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap [[arXiv](https://arxiv.org/abs/2008.01475)]
    - DARTS: Differentiable Architecture Search [[arXiv](https://arxiv.org/abs/1806.09055)]
    - UNAS: Differentiable Architecture Search Meets Reinforcement Learning [[arXiv](https://arxiv.org/abs/1912.07651)]
    - Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436)]
- Meta-Learning
    - M-NAS: Meta Neural Architecture Search [[paper](https://ojs.aaai.org//index.php/AAAI/article/view/6084)]
    - Meta-Learning of Neural Architectures for Few-Shot Learning [[arXiv](https://arxiv.org/abs/1911.11090)]
    - Auto-Meta: Automated Gradient Based Meta Learner Search [[arXiv](https://arxiv.org/abs/1806.06927)]
- Computer Vision
    - U-Net: Convolutional Networks for Biomedical Image Segmentation [[arXiv](https://arxiv.org/abs/1505.04597)]
    - ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness [[arXiv](https://arxiv.org/abs/1811.12231)]
    - On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location [[arXiv](https://arxiv.org/abs/2003.07064)]
- Reinforcement Learning
    - Recurrent experience replay in distributed Reinforcement Learning [[paper](https://openreview.net/pdf?id=r1lyTjAqYX)]
    - Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions [[arXiv](https://arxiv.org/abs/2003.08536) [blog](https://eng.uber.com/poet-open-ended-deep-learning/)]
    - Memory-based control with recurrent neural networks [[arXiv](https://arxiv.org/abs/1512.04455)]
    - Soft Actor-Critic Algorithms and Applications [[arXiv](https://arxiv.org/abs/1812.05905)]
    - Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor [[arXiv](https://arxiv.org/abs/1801.01290)]
- Can recurrent neural networks warp time? [[arXiv](https://arxiv.org/pdf/1804.11188.pdf)]

#### 2021-03
- Reinforcement Learning
    - Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning [[arXiv](https://arxiv.org/abs/1910.10897)]
    - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation [[arXiv](https://arxiv.org/abs/1708.05144)]
    - Asynchronous Methods for Deep Reinforcement Learning [[arXiv](https://arxiv.org/abs/1602.01783)]
    - Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions [[arXiv](https://arxiv.org/abs/1901.01753)]

#### 2021-02
- Meta-Learning
    - Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables [[arXiv](https://arxiv.org/abs/1903.08254)]

#### 2021-01
- Neural Architecture Search
    - A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions [[arXiv](https://arxiv.org/abs/2006.02903)]

#### 2020-12
- Meta-Learning
    - Meta-Learning in Neural Networks: A Survey [[arXiv](https://arxiv.org/abs/2004.05439)]
- Reinforcement Learning
    - Asynchronous Methods for Deep Reinforcement Learning [[arXiv](https://arxiv.org/abs/1602.01783)]
    - Effective Policy Gradient Search for Reinforcement Learning Through NEAT Based Feature Extraction [[paper](https://link.springer.com/chapter/10.1007/978-3-319-68759-9_39)]
    - Random Error Sampling-based Recurrent Neural Network Architecture Optimization [[arXiv](https://arxiv.org/abs/1909.02425)]

#### 2020-11
- Reinforcement Learning
    - Human-level control through deep reinforcement learning [[paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)]
    - Deep Reinforcement Learning with Double Q-learning [[arXiv](https://arxiv.org/abs/1509.06461)]
    - Playing Atari with Deep Reinforcement Learning [[arXiv](https://arxiv.org/abs/1312.5602)]
    - Behavior Suite for Reinforcement Learning [[paper](https://arxiv.org/1908.03568)] 
- Statistical Modeling: The Two Cultures [[paper](http://www2.math.uu.se/~thulin/mm/breiman.pdf)]

#### 2020-10
- Meta-Learning
    - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [[arXiv](https://arxiv.org/abs/1703.03400)]
    - Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples [[arXiv](https://arxiv.org/abs/1903.03096)]
    - [Reinforcement Learning, Fast and Slow](notes/RL-fast-and-slow.md) [[cell](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0)]
- The Bitter Lesson [[article](http://incompleteideas.net/IncIdeas/BitterLesson.html)]
- Weight Agnostic Neural Networks [[arXiv](https://arxiv.org/abs/1906.04358)] [[blog](https://gomerudo.github.io/ml-summaries/2020/03/09/wanns/)]

#### 2020-09
- [On First-Order Meta-Learning Algorithms](notes/First-order-meta-learning.md) [[arXiv](https://arxiv.org/abs/1801.02999)]
- [The Sorcerer’s Apprentice Guide to Training LSTMs](notes/Guide-to-training-LSTMs.md) [[blog](https://www.niklasschmidinger.com/posts/2020-09-09-lstm-tricks/)]
- [Learning to Reinforcement learn](notes/Learning-to-Reinforcement-learn.md) [[arXiv](https://arxiv.org/pdf/1611.05763.pdf)]
- [RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning](notes/RL2-fast-rl-via-slow-rl.md) [[openReview](https://openreview.net/pdf?id=HkLXCE9lx)]
- [Practical Block-wise Neural Network Architecture Generation](notes/Block-NAS-generation.md) [[arXiv](https://arxiv.org/abs/1708.05552)]

#### 2020-08
- [Deep Reinforcement Learning: Pong from Pixels](notes/DRL-pong-from-pixels.md) [[blog](http://karpathy.github.io/2016/05/31/rl/)]
- [Exploration Strategies in Deep Reinforcement Learning](notes/Exploration-strats-in-DRL.md) [[blog](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html)]
- [Learning to reinforcement learn for Neural Architecture Search](notes/Learn-2-learn-NAS.md) [[arXiv](https://arxiv.org/abs/1911.03769)]
- [Meta Reinforcement Learning](notes/MetaRL-lilian-blog.md) [[blog](https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html)]
- [Neural Architecture Search](notes/NAS-lilian-blog.md) [[blog](https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html)]
